{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "cell_execution_strategy": "setup"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "UoMR0wNvWPby"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   GPU: Go to Edit -> Notebook settings. Choose GPU as Hardware accelerator.\n",
        "*   Packages: Import some useful packages and set everything up."
      ],
      "metadata": {
        "id": "MYy27UODWTt2"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xk4idOhEPQ52"
      },
      "source": [
        "%matplotlib inline\n",
        "import copy\n",
        "import math\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import unittest\n",
        "\n",
        "from collections import Counter\n",
        "from datetime import datetime\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "def set_seed(seed):  # For reproducibility, fix random seeds.\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "\n",
        "set_seed(42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-4pcV7uPQ54"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMzROwyIPQ54"
      },
      "source": [
        "Download [SST-2](https://dl.fbaipublicfiles.com/glue/data/SST-2.zip), which is a version of [Stanford Sentiment Treebank](https://nlp.stanford.edu/sentiment/index.html) prepared by the [GLUE Benchmark](https://gluebenchmark.com/) for sentence-level binary sentiment classification of movie reviews (either positive or negative). We will assume that we have the directory `data/SST-2/` in our Google Drive account. Let's load the data and stare at it.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "InLe59yaSI2h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "675e1f7e-4981-4b5a-9e44-6cce6190bfa7"
      },
      "source": [
        "# Load the Drive helper and mount. You will have to authorize this operation.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBS_B0PSPQ54",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ed11429-6416-4f08-c741-b876fa4b4337"
      },
      "source": [
        "label2str = {0: 'NEGATIVE', 1: 'POSITIVE'}  # Integer-string mapping for labels\n",
        "\n",
        "def load_data(filename):\n",
        "  with open(os.path.join('/content/drive/My Drive/data/SST-2/', filename)) as f:\n",
        "    f.readline()\n",
        "    data = [line.split('\\t') for line in f]\n",
        "  data = [(x.split(), int(y)) for (x, y) in data]  # Text already tokenized (woohoo!), just use whitespace\n",
        "  balance = len([_ for x, y in data if y == 1]) / len(data) * 100.  # What percentage is positive?\n",
        "  return data, balance, [len(x) for x, _ in data]\n",
        "\n",
        "train_data, balance_train, lengths_train = load_data('train.tsv')\n",
        "val_data, balance_val, lengths_val = load_data('dev.tsv')\n",
        "\n",
        "print('{} train examples ({:.1f}% positive)'.format(len(train_data), balance_train))\n",
        "print('{} val examples ({:.1f}% positive)'.format(len(val_data), balance_val))\n",
        "print('No test labels released\\n')\n",
        "\n",
        "print('Some cherry-picked examples (labels {}):'.format(str(label2str)))\n",
        "for i in [13, 901, 903, 1001, 61]:\n",
        "  print('{}\\t\"{}\"'.format(train_data[i][1], ' '.join(train_data[i][0])))\n",
        "\n",
        "print('\\nSentence lengths')\n",
        "print('  Train: average {:5.1f}, max {}, min {}'.format(sum(lengths_train) / len(lengths_train), max(lengths_train), min(lengths_train)))\n",
        "print('  Val:   average {:5.1f}, max {}, min {}'.format(sum(lengths_val) / len(lengths_val), max(lengths_val), min(lengths_val)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "67349 train examples (55.8% positive)\n",
            "872 val examples (50.9% positive)\n",
            "No test labels released\n",
            "\n",
            "Some cherry-picked examples (labels {0: 'NEGATIVE', 1: 'POSITIVE'}):\n",
            "0\t\"saw how bad this movie was\"\n",
            "1\t\"nicely done\"\n",
            "0\t\"the satire is just too easy to be genuinely satisfying .\"\n",
            "1\t\"a gritty police thriller with all the dysfunctional family dynamics one could wish for\"\n",
            "0\t\"no apparent joy\"\n",
            "\n",
            "Sentence lengths\n",
            "  Train: average   9.4, max 52, min 1\n",
            "  Val:   average  19.5, max 47, min 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzzhLbeaPQ55"
      },
      "source": [
        "Feel free to look at other examples yourself. Some observations about the data:\n",
        "\n",
        "- Many sentences can be classified correctly by keywords like \"bad\" for negative and \"nicely\" for positive, so we expect the bag-of-words representation to be performant. But there are harder examples, like \"too easy to be genuinely satisfying\" and \"no apparent joy\" where the positive sentiment is flipped and \"dysfunctional\" which is a negative word but actually used for praising the movie.\n",
        "\n",
        "- Sentences are really short. In the training set, the maximum length is merely 52, and on average a sentence has less than 10 tokens. This will be convenient for training.\n",
        "\n",
        "As a first step, let's construct a vocabulary and convert everything to integers. We will introduce an \"unk\" type to represent any unknown word type at test time. We will also introduce a special padding token \"pad\", and ensure that it gets index 0 which is convenient for later use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIupVnRDPQ55",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d9892ee-48b7-40a3-fddb-0a4e655d501a"
      },
      "source": [
        "PAD = '<pad>'\n",
        "UNK = '<unk>'\n",
        "vocab = Counter([tok for toks, _ in train_data for tok in toks])\n",
        "assert not PAD in vocab\n",
        "assert not UNK in vocab\n",
        "vocab[PAD] = 9999999  # PAD will get index 0\n",
        "vocab[UNK] = 9999998  # UNK will get index 1\n",
        "vocab_size = 10000\n",
        "vocab = [word for word, _ in vocab.most_common(vocab_size)]\n",
        "assert vocab[0] == PAD\n",
        "print('Vocab size: {} (with PAD and UNK added)'.format(len(vocab)))\n",
        "print('vocab[0]:', vocab[0])\n",
        "print('vocab[1]:', vocab[1])\n",
        "w2i = {}\n",
        "for i, word in enumerate(vocab):\n",
        "  w2i[word] = i\n",
        "\n",
        "# Note that we're preserving word ordering.\n",
        "train_sents = [[w2i[tok] if tok in w2i else w2i[UNK] for tok in x] for x, _ in train_data]\n",
        "val_sents = [[w2i[tok] if tok in w2i else w2i[UNK] for tok in x] for x, _ in val_data]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab size: 10000 (with PAD and UNK added)\n",
            "vocab[0]: <pad>\n",
            "vocab[1]: <unk>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q07N89zRPQ56"
      },
      "source": [
        "Now we package data for the [PyTorch](https://pytorch.org/) library.\n",
        "\n",
        "Read [torch.utils.data](https://pytorch.org/docs/stable/data.html) for details of how PyTorch expects data. The first thing we need is a `Dataset`. This simply wraps data and returns specified items at given indices. To make items easily batchable, we will do naive padding and make every sentence have the same length. This naive padding won't work in general when sentences can be long; in such a case we need to define a custom batching operation (see below)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOU8sM1YeCba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61296af4-3d23-495a-ad18-48f4a23e2647"
      },
      "source": [
        "class SSTDataset(Dataset):  # A child class of torch.utils.data.Dataset\n",
        "\n",
        "  def __init__(self, sents, labels, max_length):\n",
        "    self.sents = sents\n",
        "    self.labels = labels\n",
        "    self.max_length = max_length\n",
        "\n",
        "  def __len__(self):  # This defines the \"size\" of the dataset.\n",
        "    return len(self.sents)\n",
        "\n",
        "  def __getitem__(self, index):  # This returns a single indexed example.\n",
        "    sent = torch.tensor(self.sents[index])\n",
        "    sent_padded = torch.cat([sent, torch.zeros(self.max_length - len(sent))]).long()  # Avoid for loop by using torch.zeros.\n",
        "    label = torch.tensor(self.labels[index])\n",
        "    return sent_padded, label, len(sent)  # Since we've padded, we need to inform the original length.\n",
        "\n",
        "dataset_train = SSTDataset(train_sents, [y for _, y in train_data], max(lengths_train))\n",
        "dataset_val = SSTDataset(val_sents, [y for _, y in val_data], max(lengths_val))\n",
        "\n",
        "x1, y1, length1 = dataset_train[0]\n",
        "print(x1, x1.size(), y1, length1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([4457,   94,    1,   37,    2, 7261, 9001,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0]) torch.Size([52]) tensor(0) 7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUhowr_xPQ56"
      },
      "source": [
        "Each element of a `Dataset` is a list of PyTorch tensor types, and can be fed into a `DataLoader` which does batching for us. **Important**: No matter what dataset you work on in the future, you will always follow this general syntax:\n",
        "```python\n",
        "loader = DataLoader(dataset,\n",
        "                    batch_size=batch_size,\n",
        "                    shuffle=shuffle,  # True if this is a loader for training\n",
        "                    num_workers=num_workers,  # Number of threads to parallelize data processing work\n",
        "                    collate_fn=collate_fn,  # Custom batching operation (e.g., needed if sequences have different lengths)\n",
        "                    batch_sampler=batch_sampler)  # Custom batch sampling\n",
        "```\n",
        "In our case, every padded sentence has the same length, so default batching kicks in and we don't have to provide a custom batching operation (`collate_fn`). A `DataLoader` is a [generator](https://wiki.python.org/moin/Generators) that allows us to iterate over data. ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSqISOC4mziW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77a82bff-5661-459c-9afa-2b4d0bb82fff"
      },
      "source": [
        "# We will define a train loader later since we'll be changing training batch sizes.\n",
        "dataloader_val = DataLoader(dataset_val, batch_size=128, shuffle=False)\n",
        "\n",
        "for batch_num, batch in enumerate(dataloader_val):\n",
        "  sents, labels, lengths = batch\n",
        "  if batch_num == 0:\n",
        "    print(sents, sents.size())  # (batch_size, max_length)\n",
        "    print(labels, labels.size())  # batch_size\n",
        "    print(lengths, lengths.size())  # batch_size\n",
        "  # Don't break here, so the generator is all spent and reset."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[  13,    9,    4,  ...,    0,    0,    0],\n",
            "        [   1, 3043,    5,  ...,    0,    0,    0],\n",
            "        [ 813,   93,    8,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [   4,   20,   11,  ...,    0,    0,    0],\n",
            "        [   1,    2, 7356,  ...,    0,    0,    0],\n",
            "        [  12,    2,  192,  ...,    0,    0,    0]]) torch.Size([128, 47])\n",
            "tensor([1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,\n",
            "        1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,\n",
            "        1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,\n",
            "        1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,\n",
            "        0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,\n",
            "        1, 0, 1, 1, 1, 1, 0, 0]) torch.Size([128])\n",
            "tensor([ 9,  4, 20, 20,  9, 21,  5, 10, 20, 33, 16, 30, 19, 23, 25, 20, 18, 32,\n",
            "        10, 19, 32, 13,  6, 16, 16, 27, 28, 23, 10, 11, 28, 25,  8, 33, 20, 11,\n",
            "        26, 27, 23, 16, 19, 21, 21, 11, 19, 14, 17, 12, 12, 20, 35, 19, 18, 14,\n",
            "        21,  7, 22, 11,  9, 32, 21, 28, 19, 26, 16, 16, 17,  9,  6, 14, 27, 28,\n",
            "         8, 24, 11, 10, 13, 16, 31, 44, 18, 24, 14, 23, 23, 22, 21, 17, 28, 19,\n",
            "        16, 19, 13, 16, 24, 26, 28, 23, 21, 10, 22, 21, 15, 14, 47, 32, 16, 21,\n",
            "        20, 17, 11, 16,  5,  5, 29, 16, 16, 26, 20, 14, 28, 24,  6,  9, 33, 25,\n",
            "        22, 35]) torch.Size([128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1rItePFPQ56"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMZa5WpkPQ57"
      },
      "source": [
        "Let's write code to train a binary classifier. We will again train it by the cross-entropy loss. As an exercise, we will implement the loss module by hand, which combines sigmoid with log for a more numerically stable gradient form. See the [page about extending torch.autograd](https://pytorch.org/docs/master/notes/extending.html#extending-torch-autograd) to understand the syntax here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lEjAGSZ_PQ57"
      },
      "source": [
        "\n",
        "class BinaryCrossEntropyLossFunction(torch.autograd.Function):\n",
        "\n",
        "  @staticmethod\n",
        "  def forward(ctx, logits, labels):\n",
        "    probs = 1. / (1 + (-logits).exp())\n",
        "    ctx.save_for_backward(probs, labels)  # Just need probabilities for backward\n",
        "    losses = - (labels * torch.log(probs) + (1 - labels) * torch.log(1 - probs))  # By the definition of binary cross entropy loss\n",
        "    return losses.sum()\n",
        "\n",
        "  @staticmethod\n",
        "  def backward(ctx, grad_output):\n",
        "    probs, labels = ctx.saved_tensors\n",
        "    jacobian = probs - labels  # Jacobian is partial derivative of Loss function wrt logits. By chain rule we get the Jacobian as p - y i.e. probs - labels\n",
        "    grad_logits = grad_output * jacobian\n",
        "    return grad_logits, None  # No need to calculate gradient wrt labels\n",
        "\n",
        "\n",
        "class BinaryCrossEntropyLoss(nn.Module):\n",
        "\n",
        "  def forward(self, logits, labels):\n",
        "    return BinaryCrossEntropyLossFunction.apply(logits, labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDnTq44bPQ57"
      },
      "source": [
        "Now our custom `BinaryCrossEntropyLoss` module can be plugged into a computation graph as a node. When the scalar output node of a computation graph calls `backward()`, the graph will automatically run the forward and backward pass.\n",
        "\n",
        "For instance, in the unit test and also training below, we will call backward on `loss / N`. Behind the hood, PyTorch creates another node with the scalar division that has its own forward and backward functions when we write `loss / N` which is syntactic sugar."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFAZwECkPQ58",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91318fd3-4b5f-4589-e2c1-9a58a7172d73"
      },
      "source": [
        "class TestBinaryCrossEntropyLoss(unittest.TestCase):\n",
        "\n",
        "  def setUp(self):\n",
        "    self.batch_size = 42\n",
        "    self.places = 6\n",
        "    self.logits = np.random.randn(self.batch_size)\n",
        "    self.labels = np.random.randint(2, size=self.batch_size)\n",
        "    self.mine = BinaryCrossEntropyLoss()\n",
        "    self.gold = torch.nn.BCEWithLogitsLoss(reduction='sum')\n",
        "\n",
        "  def test_forward_backward(self):\n",
        "    def run_loss_layer(layer):\n",
        "      variables = torch.tensor(self.logits, requires_grad=True)\n",
        "\n",
        "      # The final node is actually a scalar division node.\n",
        "      loss_node = layer(variables, torch.tensor(self.labels).float()) / self.batch_size\n",
        "\n",
        "      # This runs forward backward. Our layer will propagate gradient from the final node (scalar division).\n",
        "      loss_node.backward()\n",
        "\n",
        "      grad = copy.deepcopy(variables.grad)\n",
        "      return loss_node.item(), grad.tolist()\n",
        "\n",
        "    loss, grad = run_loss_layer(self.mine)\n",
        "    loss_gold, grad_gold = run_loss_layer(self.gold)\n",
        "    self.assertAlmostEqual(loss, loss_gold, places=self.places)\n",
        "    for i in range(len(grad)):\n",
        "        self.assertAlmostEqual(grad[i], grad_gold[i], places=self.places)\n",
        "\n",
        "unittest.main(TestBinaryCrossEntropyLoss(), argv=[''], verbosity=2, exit=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "test_forward_backward (__main__.TestBinaryCrossEntropyLoss) ... ok\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 1 test in 0.118s\n",
            "\n",
            "OK\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<unittest.main.TestProgram at 0x7ce3fc589c60>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQEgkzhXPQ58"
      },
      "source": [
        "A general binary classifier module is implemented below. It assumes an **encoder** module that encodes sentences into same-dimensional embeddings. At that point, all that's left is to apply a linear classifier to get logits and use them to classify / compute loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQwqGPTWPQ59"
      },
      "source": [
        "class BinaryClassifier(nn.Module):\n",
        "\n",
        "  def __init__(self, encoder):\n",
        "    super().__init__()\n",
        "    self.encoder = encoder\n",
        "    self.score = nn.Linear(encoder.dim, 1)  # In PyTorch, Linear has both weight matrix and bias (W, b) by default.\n",
        "    self.loss = BinaryCrossEntropyLoss()\n",
        "\n",
        "  def forward(self, sents, lengths, labels=None):\n",
        "    embs = self.encoder(sents, lengths)  # (batch_size, dim)\n",
        "    logits = self.score(embs)  # (batch_size, 1)\n",
        "    logits = logits.squeeze(1)  # batch_size\n",
        "    loss_total = None if labels is None else self.loss(logits, labels)\n",
        "    return logits, loss_total"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQHSpAaKPQ5-"
      },
      "source": [
        "## Continuous bag-of-words (CBOW) encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7yahQDUPQ5-"
      },
      "source": [
        "We will consider a simple encoder (we will call this \"continuous bag-of-words\" or CBOW encoder) that averages word embeddings, and optionally applies a nonlinear feedforward layer with a residual connection."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nz5M7mqQPQ5-"
      },
      "source": [
        "class CBOWEncoder(nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size, dim, ff=False, activation='relu', drop=0.):\n",
        "    super().__init__()\n",
        "    self.dim = dim\n",
        "    self.ff = ff\n",
        "    self.wemb = nn.Embedding(vocab_size, dim, padding_idx=0)  # Assumes padding token has index 0\n",
        "    if ff:\n",
        "      # Chain a linear layer with an activation layer into one layer.\n",
        "      self.ff_layer = nn.Sequential(nn.Linear(dim, dim),\n",
        "                                    {'relu': nn.ReLU(), 'tanh': nn.Tanh()}[activation])\n",
        "    self.drop = nn.Dropout(drop)\n",
        "\n",
        "  def forward(self, sents, lengths):\n",
        "    wembs = self.wemb(sents)  # (batch_size, max_length, dim)\n",
        "    embs = wembs.sum(dim=1) / lengths[:, None]  # (batch_size, dim)\n",
        "    if self.ff:\n",
        "      embs = self.ff_layer(embs) + embs  # Residual connection, probably harmless\n",
        "    embs = self.drop(embs)\n",
        "    return embs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUHZsTx0ccJ3"
      },
      "source": [
        "We introduced the notion of **word embeddings** (more generally, feature embeddings). This is simply a $V \\times d$ matrix where we have a $d$-dimensional *parameter* for each of $V$ word types. We learn these embeddings (along with other weights) to minimize the loss.\n",
        "\n",
        "Because we use padding, we will indicate that 0 is a pad token. Then PyTorch Embedding will make this a zero vector, so that it does not affect the loss value or gradients.\n",
        "\n",
        "All parameter values are initialized randomly according to some default initialization scheme. For instance, the [Embedding](https://github.com/pytorch/pytorch/blob/8a9090219ef3a18d41f5b95ab545915f707847c2/torch/nn/modules/sparse.py#L136) layer uses normal and the [Linear](https://github.com/pytorch/pytorch/blob/8a9090219ef3a18d41f5b95ab545915f707847c2/torch/nn/modules/linear.py#L86) layer uses Kaiming uniform."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyrL1GSndChz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a96a6f63-3630-4b9c-8b12-e571050520d4"
      },
      "source": [
        "A = nn.Embedding(5, 2, padding_idx=0)\n",
        "print(A.weight)  # Each row corresponds to a 2-dimensional embedding of the corresponding feature type."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[ 0.0000,  0.0000],\n",
            "        [ 1.1561,  0.3965],\n",
            "        [-2.4661,  0.3623],\n",
            "        [ 0.3765, -0.1808],\n",
            "        [ 0.3930,  0.4327]], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSkDLwadzsHk"
      },
      "source": [
        "We also use **dropout**. The dropout layer `Dropout(p)` is an effective tool for regularization. During training, it \"drops\" each element of the input array to 0 with probability $p$, and for those elements not dropped scale them as $x \\mapsto x/(1-p)$ to roughly preserve the size of the input. During evaluation, PyTorch turns off dropout."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3H6DVRtztbQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9411d9e6-c0e4-47d4-a965-26858a9bc0f6"
      },
      "source": [
        "dropper = nn.Dropout(0.75)  # Train mode by default.\n",
        "x = torch.randn(10)\n",
        "print('x:          ({})'.format(' '.join(['{:.2f}'.format(val) for val in x])))\n",
        "print('dropper(x): ({})'.format(' '.join(['{:.2f}'.format(val) for val in dropper(x)])))  # Survivors multiplied by 4\n",
        "\n",
        "dropper.eval()\n",
        "print('At eval:    ({})'.format(' '.join(['{:.2f}'.format(val) for val in dropper(x)])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x:          (-1.36 1.36 0.67 -0.71 -0.33 -0.28 -0.42 -1.33 -0.36 0.15)\n",
            "dropper(x): (-0.00 0.00 0.00 -2.83 -0.00 -0.00 -0.00 -0.00 -1.46 0.00)\n",
            "At eval:    (-1.36 1.36 0.67 -0.71 -0.33 -0.28 -0.42 -1.33 -0.36 0.15)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvQVrl0UPQ5-"
      },
      "source": [
        "This encoder module is fed into the classifier module above. In PyTorch this makes the parameters of the encoder module as part of the parameters of the classifier module. You can print the model and see all parameters and their information like dimensions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wyLdqX35PQ5-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "faa59025-166c-4de9-a69d-1b01d237152e"
      },
      "source": [
        "def count_params(model):\n",
        "  return sum(p.numel() for p in model.parameters())\n",
        "\n",
        "model = BinaryClassifier(CBOWEncoder(vocab_size, 128, ff=True, activation='tanh', drop=0.1))\n",
        "print('Model has {} parameters\\n'.format(count_params(model)))\n",
        "print(model)\n",
        "print()\n",
        "\n",
        "print('First few values of the score layer\\'s weight vector')\n",
        "print(model.score.weight.data[0][:10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model has 1296641 parameters\n",
            "\n",
            "BinaryClassifier(\n",
            "  (encoder): CBOWEncoder(\n",
            "    (wemb): Embedding(10000, 128, padding_idx=0)\n",
            "    (ff_layer): Sequential(\n",
            "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
            "      (1): Tanh()\n",
            "    )\n",
            "    (drop): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (score): Linear(in_features=128, out_features=1, bias=True)\n",
            "  (loss): BinaryCrossEntropyLoss()\n",
            ")\n",
            "\n",
            "First few values of the score layer's weight vector\n",
            "tensor([-0.0363, -0.0349, -0.0250,  0.0802,  0.0356, -0.0730,  0.0248,  0.0112,\n",
            "        -0.0367, -0.0807])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IiEsRykPQ5_"
      },
      "source": [
        "Note that the number of parameters is quite large, mostly because of the word embedding matrix which alone has $Vd$ parameters. Let's write a helper function to evaluate a model on validation data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVbLWQpnngD5"
      },
      "source": [
        "def get_acc_val(model):\n",
        "  num_correct_val = 0\n",
        "  model.eval()  # This turns off the training mode.\n",
        "  with torch.no_grad():  # This deactivates autodiff for improved efficiency.\n",
        "    for batch in dataloader_val:\n",
        "      sents, labels, lengths = batch\n",
        "      sents = sents.to(model.score.weight.device)  # Send data to same device that model is on.\n",
        "      labels = labels.to(model.score.weight.device)\n",
        "      lengths = lengths.to(model.score.weight.device)\n",
        "      logits, _ = model(sents, lengths, labels)\n",
        "      preds = torch.where(logits > 0., 1, 0)  # 1 if p(1|x) > 0.5, 0 else\n",
        "      num_correct_val += (preds == labels).sum()\n",
        "  acc_val = num_correct_val / len(dataloader_val.dataset) * 100.\n",
        "  return acc_val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daAUbuXGXoTr"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vpi6jUiPQ5_"
      },
      "source": [
        "Let's again define our custom SGD optimizer to demystify the training process. It simply iterates over all parameters and takes gradient steps, assuming that the gradient of the parameter is stored in `grad` field."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLPNgoazPQ5_"
      },
      "source": [
        "class SGDOptimizer:\n",
        "\n",
        "  def __init__(self, parameters, learning_rate):\n",
        "    self.params = list(parameters)\n",
        "    self.lr = learning_rate\n",
        "\n",
        "  def step(self):\n",
        "    with torch.no_grad():\n",
        "      for p in self.params:\n",
        "        p -= self.lr * p.grad  # In PyTorch, every parameter object has a grad field that stores an accumulated gradient.\n",
        "\n",
        "  def zero_grad(self):\n",
        "    for p in self.params:\n",
        "      p.grad.data.zero_()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oW8-ofePQ5_"
      },
      "source": [
        "Let's write code for training a PyTorch model. This is a pretty general training routine, independent of specific models or tasks. Spend some time to make yourself familiar with it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ea9k8qNPQ5_"
      },
      "source": [
        "def train(model, dataloader_train, optimizer, num_epochs=10, clip=0., verbose=True, device='cpu', select_model=False):\n",
        "  model = model.to(device)  # Move the model to device.\n",
        "\n",
        "  loss_avg = float('inf')\n",
        "  acc_train = 0.\n",
        "  best_acc_val = 0.\n",
        "  start_time = datetime.now()  # Keep track of training time.\n",
        "  best_state_dict = None\n",
        "  num_continuous_fails = 0\n",
        "  tolerance = 6\n",
        "  for epoch in range(num_epochs):\n",
        "    model.train()  # This turns on the training mode (e.g., enable dropout).\n",
        "    loss_total = 0.\n",
        "    num_correct_train = 0\n",
        "    for batch_ind, batch in enumerate(dataloader_train):\n",
        "      sents, labels, lengths = batch\n",
        "      sents = sents.to(device)  # Move data to device.\n",
        "      labels = labels.to(device)\n",
        "      lengths = lengths.to(device)\n",
        "      logits, loss_batch_total = model(sents, lengths, labels)\n",
        "      preds = torch.where(logits > 0., 1, 0)  # 1 if p(1|x) > 0.5, 0 else\n",
        "      num_correct_train += (preds == labels).sum()\n",
        "      loss_total += loss_batch_total.item()\n",
        "\n",
        "      if math.isnan(loss_total):  # Let's not waste time if we get NaN.\n",
        "        break\n",
        "\n",
        "      loss_batch_avg = loss_batch_total / sents.size(0)  # Final node of the computation graph of this batch.\n",
        "      loss_batch_avg.backward()  # This calls forward and backward.\n",
        "\n",
        "      if clip > 0.:  # Optional gradient clipping\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "\n",
        "      optimizer.step()  # optimizer updates model weights based on stored gradients\n",
        "      optimizer.zero_grad()  # Reset gradient slots to zero\n",
        "\n",
        "    if math.isnan(loss_total):\n",
        "      print('Stopping training because loss is NaN')\n",
        "      break\n",
        "\n",
        "    # Useful training information, kept track of efficiently.\n",
        "    loss_avg = loss_total / len(dataloader_train.dataset)\n",
        "    acc_train = num_correct_train / len(dataloader_train.dataset) * 100.\n",
        "\n",
        "    # Check validation performance at the end of every epoch.\n",
        "    acc_val = get_acc_val(model)\n",
        "\n",
        "    if verbose:\n",
        "      print('Epoch {:3d} | avg loss {:8.4f} | train acc {:2.2f} | val acc {:2.2f}'.format(epoch + 1, loss_avg, acc_train, acc_val))\n",
        "\n",
        "    if acc_val > best_acc_val:\n",
        "      num_continuous_fails = 0\n",
        "      best_acc_val = acc_val\n",
        "      if select_model:\n",
        "        best_state_dict = copy.deepcopy(model.state_dict())\n",
        "    else:\n",
        "      num_continuous_fails += 1\n",
        "      if num_continuous_fails > tolerance:\n",
        "        print('Early stopping')\n",
        "        break\n",
        "\n",
        "  train_time = datetime.now() - start_time\n",
        "  if verbose:\n",
        "    print('Final avg loss {:8.4f} | final train acc {:2.2f} | best val acc {:2.2f} | train time {:d} secs'.format(loss_avg, acc_train, best_acc_val, train_time.seconds))\n",
        "\n",
        "  if select_model and best_state_dict is not None:\n",
        "    model.load_state_dict(best_state_dict)\n",
        "  model.eval()\n",
        "  return loss_avg, acc_train, best_acc_val, train_time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1hEJlRszA0y"
      },
      "source": [
        "Let's try training a CBOW sentiment classifier. Since we have no idea what hyperparameter configuration works, we will do a random search, treating all of the following as hyperparameters:\n",
        "- Random seed\n",
        "- Learning rate\n",
        "- Batch size\n",
        "- Dropout probability\n",
        "- Whether to use a nonlinear feedforward layer or not\n",
        "- Activation function\n",
        "- Dimension of embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-IWPWoqSUQ3M"
      },
      "source": [
        "grid = {'seed': list(range(100000)),\n",
        "        'lr': [1, 0.1, 0.01, 0.001, 0.0001],\n",
        "        'batch_size': [16, 32, 64],\n",
        "        'drop': [0., 0.1, 0.3, 0.5],\n",
        "        'ff': [False, True],\n",
        "        'activation': ['relu', 'tanh'],\n",
        "        'dim': [64, 128, 256]}\n",
        "\n",
        "def print_row(hparams, train_output):\n",
        "  loss_avg, acc_train, acc_val, train_time = train_output\n",
        "  print('seed {:6d} | lr {:1.5f} | batch {:d} | drop {:.1f} | ff {:d} | {} | dim {:3d} | loss {:1.4f} | train acc {:2.2f} | val acc {:2.2f} | {:d} secs'.format(\n",
        "      hparams['seed'], hparams['lr'], hparams['batch_size'], hparams['drop'], hparams['ff'], hparams['activation'], hparams['dim'], loss_avg, acc_train, acc_val, train_time.seconds))\n",
        "\n",
        "num_runs = 20\n",
        "\n",
        "if False:  # Set True if you want to run a random search.\n",
        "  best_acc_val = 0.\n",
        "  for run_num in range(1, num_runs + 1):\n",
        "    hparams = {}\n",
        "    for hparam, values in grid.items():\n",
        "      hparams[hparam] = random.choice(values)\n",
        "    set_seed(hparams['seed'])\n",
        "    encoder = CBOWEncoder(vocab_size, hparams['dim'], ff=hparams['ff'], activation=hparams['activation'], drop=hparams['drop'])\n",
        "    model = BinaryClassifier(encoder)\n",
        "    dataloader_train = DataLoader(dataset_train, batch_size=hparams['batch_size'], shuffle=True)\n",
        "    optimizer = SGDOptimizer(model.parameters(), hparams['lr'])\n",
        "    train_output = train(model, dataloader_train, optimizer, clip=1., num_epochs=30, verbose=False, device='cuda')\n",
        "    print_row(hparams, train_output)\n",
        "    acc_val = train_output[2]\n",
        "    if acc_val > best_acc_val:\n",
        "      best_acc_val = acc_val\n",
        "  print('Best acc {:2.2f}'.format(best_acc_val))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Igf9L_iUp0w-"
      },
      "source": [
        "You're not required to run the above search yourself (it'll take $>30$ minutes). In practice, you'd want to parallelize this so that several runs are executed at the same time (assuming you have multiple CPUs/GPUs). But you'll see something like this:\n",
        "\n",
        "```\n",
        "Stopping training because loss is NaN\n",
        "seed  83810 | lr 1.00000 | batch 16 | drop 0.3 | ff 0 | relu | dim  64 | loss 0.6967 | train acc 63.64 | val acc 68.35 | 27 secs\n",
        "Stopping training because loss is NaN\n",
        "seed  84440 | lr 1.00000 | batch 64 | drop 0.3 | ff 0 | relu | dim 256 | loss 0.5923 | train acc 72.33 | val acc 75.23 | 43 secs\n",
        "Early stopping\n",
        "seed  44131 | lr 0.00010 | batch 32 | drop 0.5 | ff 0 | tanh | dim 256 | loss 0.6894 | train acc 54.89 | val acc 55.50 | 63 secs\n",
        "seed  61748 | lr 0.10000 | batch 64 | drop 0.1 | ff 1 | relu | dim  64 | loss 0.5525 | train acc 71.66 | val acc 70.18 | 155 secs\n",
        "Early stopping\n",
        "seed  55325 | lr 0.00100 | batch 32 | drop 0.3 | ff 1 | tanh | dim 256 | loss 0.6546 | train acc 61.30 | val acc 63.53 | 130 secs\n",
        "Early stopping\n",
        "seed  38842 | lr 0.01000 | batch 32 | drop 0.5 | ff 0 | tanh | dim  64 | loss 0.6771 | train acc 56.96 | val acc 55.39 | 94 secs\n",
        "Stopping training because loss is NaN\n",
        "seed  31506 | lr 1.00000 | batch 32 | drop 0.5 | ff 1 | relu | dim 256 | loss inf | train acc 0.00 | val acc 0.00 | 2 secs\n",
        "Early stopping\n",
        "seed  38316 | lr 0.00100 | batch 64 | drop 0.0 | ff 0 | tanh | dim  64 | loss 0.6819 | train acc 55.78 | val acc 53.33 | 47 secs\n",
        "Stopping training because loss is NaN\n",
        "seed  53981 | lr 1.00000 | batch 32 | drop 0.5 | ff 1 | tanh | dim  64 | loss 0.2527 | train acc 90.07 | val acc 81.08 | 124 secs\n",
        "Stopping training because loss is NaN\n",
        "seed  76179 | lr 0.01000 | batch 16 | drop 0.0 | ff 1 | relu | dim 256 | loss 0.4118 | train acc 83.50 | val acc 71.90 | 208 secs\n",
        "Early stopping\n",
        "seed  95698 | lr 0.00100 | batch 32 | drop 0.5 | ff 0 | tanh | dim  64 | loss 0.6816 | train acc 55.82 | val acc 53.10 | 56 secs\n",
        "Stopping training because loss is NaN\n",
        "seed  36782 | lr 1.00000 | batch 32 | drop 0.0 | ff 1 | tanh | dim 128 | loss 0.3386 | train acc 85.51 | val acc 75.57 | 57 secs\n",
        "Stopping training because loss is NaN\n",
        "seed  74559 | lr 1.00000 | batch 16 | drop 0.0 | ff 0 | tanh | dim 256 | loss inf | train acc 0.00 | val acc 0.00 | 2 secs\n",
        "Early stopping\n",
        "seed  88667 | lr 0.00100 | batch 16 | drop 0.5 | ff 0 | tanh | dim 256 | loss 0.6634 | train acc 60.00 | val acc 60.78 | 283 secs\n",
        "Stopping training because loss is NaN\n",
        "seed  28082 | lr 0.10000 | batch 16 | drop 0.0 | ff 1 | tanh | dim 128 | loss 0.3180 | train acc 86.37 | val acc 73.97 | 266 secs\n",
        "Stopping training because loss is NaN\n",
        "seed  51128 | lr 1.00000 | batch 16 | drop 0.3 | ff 0 | tanh | dim 256 | loss inf | train acc 0.00 | val acc 0.00 | 4 secs\n",
        "seed  73733 | lr 0.00100 | batch 64 | drop 0.1 | ff 1 | relu | dim 128 | loss 0.6679 | train acc 59.05 | val acc 57.45 | 144 secs\n",
        "Early stopping\n",
        "seed  38556 | lr 0.00100 | batch 32 | drop 0.3 | ff 1 | relu | dim  64 | loss 0.6774 | train acc 57.30 | val acc 54.36 | 134 secs\n",
        "seed  81377 | lr 0.00100 | batch 16 | drop 0.3 | ff 1 | relu | dim 128 | loss 0.6661 | train acc 59.61 | val acc 59.52 | 384 secs\n",
        "seed  81160 | lr 0.00100 | batch 16 | drop 0.0 | ff 0 | tanh | dim 128 | loss 0.6613 | train acc 60.71 | val acc 61.58 | 329 secs\n",
        "Best acc 81.08\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hQGNRVKhLmq"
      },
      "source": [
        "It can be overwhelming with so many moving parts. Nonetheless, we can make some observations:\n",
        "\n",
        "1. **The performance varies wildly.** Don't be discouraged (yet)! It is expected that many hyperparameter values are completely off, and only a very small subset of them work at all. If we find *some* configuration that seems to work generally okay (e.g., learning rate 1 and batch size 32), it doesn't matter how many of these random runs are failures.\n",
        "\n",
        "2. **Numerical stability is an issue.** In many cases we get NaN loss because the learning rate is too high. But then the performance is often bad because the learning rate is too low. Optimization issues can be mitigated to some degree by using a more robust optimizer like [Adam](https://pytorch.org/docs/stable/optim.html#torch.optim.Adam), but are in general unavoidable.\n",
        "\n",
        "3. **It's pretty difficult to tell which hparam is having what impact.** Because the hparams interact, it's often hard to tell what's actually making a difference. For instance, in the above run we got lucky with val acc 81.08. Is it because of the learning rate 1, or the batch size 32, or the use of feedforward layer with tanh, or the dimension 64, or the dropout prob 0.5, or even the random seed 53981? There's no magical solution, but we can still try to get a big picture, for instance (1) basic SGD doesn't seem to work well with learning rates that are too small (e.g., $0.001$), (2) additional nonlinaer feedforward doesn't seem that beneficial (we can get 75.23 acc without it). To tell the impact of a specific hparam, we must do a controlled study (e.g., does dropout help?).\n",
        "\n",
        "Some advice:\n",
        "\n",
        "- If well-working hyperparameter values are known (e.g., you're replicating some paper), **start from the reported hyperparameter values** instead of searching from scratch like this. It'll save you a huge amount of time and effort.\n",
        "\n",
        "- To avoid exaggerating lucky performance, you may want to compute standard deviation across ~10 seed values, or even do a [statistical significance test](http://karlstratos.com/notes/statsig.pdf). Unless your new model works *substantially* better than baselines, it's important to be wary of randomness in performance.\n",
        "\n",
        "We will just use the best configuration based on the search above and see what we get."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0PbqdMgqPX3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdc7a10e-44b0-4902-aec1-cd167df20b3f"
      },
      "source": [
        "if True: # Set True to run.\n",
        "  set_seed(53981)\n",
        "  model = BinaryClassifier(CBOWEncoder(vocab_size, 64, ff=True, activation='tanh', drop=0.5))\n",
        "  print('Model has {} parameters\\n'.format(count_params(model)))\n",
        "  _ = train(model, DataLoader(dataset_train, batch_size=32, shuffle=True),\n",
        "            SGDOptimizer(model.parameters(), 1.), clip=1., num_epochs=30, verbose=True, device='cuda')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model has 644225 parameters\n",
            "\n",
            "Epoch   1 | avg loss   0.7385 | train acc 55.19 | val acc 59.29\n",
            "Epoch   2 | avg loss   0.6818 | train acc 61.58 | val acc 69.50\n",
            "Epoch   3 | avg loss   0.6211 | train acc 67.80 | val acc 72.82\n",
            "Epoch   4 | avg loss   0.5605 | train acc 72.59 | val acc 75.46\n",
            "Epoch   5 | avg loss   0.5087 | train acc 76.38 | val acc 76.83\n",
            "Epoch   6 | avg loss   0.4610 | train acc 79.39 | val acc 77.64\n",
            "Epoch   7 | avg loss   0.4193 | train acc 81.68 | val acc 76.49\n",
            "Epoch   8 | avg loss   0.3840 | train acc 83.69 | val acc 79.36\n",
            "Epoch   9 | avg loss   0.3548 | train acc 85.30 | val acc 77.98\n",
            "Epoch  10 | avg loss   0.3305 | train acc 86.48 | val acc 79.24\n",
            "Epoch  11 | avg loss   0.3101 | train acc 87.42 | val acc 78.21\n",
            "Epoch  12 | avg loss   0.2925 | train acc 88.27 | val acc 80.05\n",
            "Epoch  13 | avg loss   0.2771 | train acc 89.05 | val acc 79.24\n",
            "Epoch  14 | avg loss   0.2637 | train acc 89.39 | val acc 78.90\n",
            "Epoch  15 | avg loss   0.2540 | train acc 90.00 | val acc 80.62\n",
            "Stopping training because loss is NaN\n",
            "Final avg loss   0.2540 | final train acc 90.00 | best val acc 80.62 | train time 108 secs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HjVPgBmYnLF"
      },
      "source": [
        "So what is the conclusion? We can't say for certain, since there *might* be some magical hyperparameter value that we didn't use, but we can say with some confidence that the CBOW model can get roughly **70-80% validation accuracy** with a right configuration. It's unlikely that we will dramatically improve over this (e.g., 90%) without fundamentally changing the model, using the same amount of labeled data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pwTI-jgsWbm"
      },
      "source": [
        "## Convolutional neural network (CNN) encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xc_3TD6ClXc"
      },
      "source": [
        "A convolutional layer takes a multi-dimensional array and slides $K$ distinct learnable tensors (called filters or kernels) to induce $K \\times S$ scalar values where $S$ is the number of slides (each scalar is the sum of an elementwise product). Then, for each filter vector containing $S$ values we take max (called max pooling), resulting in a final $K$-dimensional embedding. This can be seen as learning a set of small \"windows\" and for each window taking the most activated slide in the input. CNN is a foundational tool in image processing: see [here](https://cs231n.github.io/convolutional-networks/#conv) for a tutorial on general CNNs. It is straightforward to apply CNNs to text by treating a sentence as a 2-dimensional array ($1 \\times d$ where $d$ is the word embedding dimension).\n",
        "\n",
        "In PyTorch, we have a prebuilt conv layer `Conv2d` which makes sure that everything is done as efficiently as possible (e.g., parallelize the sliding operation). So we'll use this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Poy7Kdfw1wkK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8074108a-4901-4f87-ad91-82c6646d3d61"
      },
      "source": [
        "dim_word = 10\n",
        "num_filters = 3\n",
        "filter_width = 4\n",
        "filter_height = dim_word\n",
        "A = nn.Embedding(5, dim_word, padding_idx=0)\n",
        "x = A(torch.LongTensor([[1, 2, 3, 4, 0, 0, 0],\n",
        "                        [3, 3, 2, 2, 2, 1, 1]]))  # (batch_size, length, dim_word)\n",
        "\n",
        "# Make \"height 1\" explicit\n",
        "x = x.unsqueeze(1)  # (batch_size, 1, length, dim_word)\n",
        "\n",
        "# The last dim of conv output is dim_word - filter_height + 1, which will be 1 in our case.\n",
        "conv2 = nn.Conv2d(1, num_filters, (filter_width, filter_height))\n",
        "relu = nn.ReLU()\n",
        "h = conv2(x)  # (batch_size, num_filters, length - filter_width + 1, 1)\n",
        "h = h.squeeze(-1)  # (batch_size, num_filters, length - filter_width + 1)\n",
        "h = relu(h)\n",
        "\n",
        "# Max pooling in this context amounts to taking max along the last axis.\n",
        "z = h.max(dim=-1)[0]  # (batch_size, num_filters)\n",
        "\n",
        "print('x ', tuple(x.size()))\n",
        "print('h ', tuple(h.size()))\n",
        "print('z ', tuple(z.size()))  # (num_filters)-dimensional embedding of each element in batch"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x  (2, 1, 7, 10)\n",
            "h  (2, 3, 4)\n",
            "z  (2, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "271BehOgEHGS"
      },
      "source": [
        "We learn *multiple* such filters with $W$ different widths (e.g., 3, 4, 5, corresponding to trigrams, 4-grams, 5-grams) and concatenate their outputs, so in the end we represent each sentence by a $KW$-dimensional vector.\n",
        "- **Pros**: Simple. Good at identifying local patterns ($n$-grams).\n",
        "- **Cons**: The output of a CNN is still not a function of the order of the entire sequence. Thus it cannot capture long-distance dependencies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9ptWg_zFNlA"
      },
      "source": [
        "class CNNEncoder(nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size, dim_word=300, num_filters=100, filter_widths=[3, 4, 5], drop=0.5):\n",
        "    super().__init__()\n",
        "    self.dim = num_filters * len(filter_widths)  # This is the final dimension of a sentence embedding.\n",
        "    self.wemb = nn.Embedding(vocab_size, dim_word, padding_idx=0)  # Assumes padding token has index 0\n",
        "\n",
        "    # Convolutional layers corresponding to different widths.\n",
        "    self.convs = nn.ModuleList([nn.Conv2d(1, num_filters, (filter_width, dim_word)) for filter_width in filter_widths])\n",
        "\n",
        "    self.relu = nn.ReLU()\n",
        "    self.drop = nn.Dropout(drop)\n",
        "\n",
        "  def forward(self, sents, lengths):\n",
        "\n",
        "    # Adding a channel dimension\n",
        "    sents = sents.unsqueeze(1)\n",
        "\n",
        "    # Embed words in sentences.\n",
        "    wembs = self.wemb(sents)\n",
        "\n",
        "    #  and applying convolutional layers.\n",
        "    conv_outs = [conv(wembs) for conv in self.convs]\n",
        "\n",
        "    # Squeezing the last dimension to remove it after convolution.\n",
        "    outs = [out.squeeze(-1) for out in conv_outs]\n",
        "\n",
        "    # Applying ReLU activation.\n",
        "    relu_outs = [self.relu(out) for out in outs]\n",
        "\n",
        "    # Applying max pooling across the output_length dimension to extract the most significant features.\n",
        "    pool_outs = [out.max(dim=-1)[0] for out in relu_outs]\n",
        "\n",
        "    # Concatenating pooled outputs from all convolutional layers.\n",
        "    concat_outs = torch.cat(pool_outs, dim=1)\n",
        "\n",
        "    # Applying dropout to the concatenated features.\n",
        "    embs = self.drop(concat_outs)\n",
        "\n",
        "    return embs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vw8wphlQlTYO"
      },
      "source": [
        "Let's try out the CNN encoder! To avoid the hassle of hyperparameter tuning as much as possible, we will mostly use reported values (e.g., filter widths [3, 4, 5], dropout 0.5, etc.) in [Kim's paper](https://arxiv.org/pdf/1408.5882.pdf) which reported one of the first successful applications of CNN for NLP. We'll also use PyTorch's predefined [Adam optimizer](https://arxiv.org/pdf/1408.5882.pdf) which uses a moving average of the gradient (i.e., momentum) and also normalizes it by an approximation of standard deviation. Adam is often more stable and converges faster than basic SGD so it's become a standard optimizer.\n",
        "\n",
        "The configuration below was identified by just trying some perturbations of hyperparameter values that seemed to work well. In particular, Adam's [weight decay/$l_2$ regularization](https://arxiv.org/pdf/1711.05101.pdf) was helpful in using a larger learning rate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JoWVkVm_rbNC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33709d63-4bbf-4a0c-8ea9-6638ae9f5a32"
      },
      "source": [
        "if True:  # Set True to run.\n",
        "  set_seed(42)\n",
        "  model = BinaryClassifier(CNNEncoder(vocab_size))\n",
        "  print('Model has {} parameters\\n'.format(count_params(model)))\n",
        "  _ = train(model, DataLoader(dataset_train, batch_size=64, shuffle=True),\n",
        "            torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0003), clip=0., num_epochs=30, verbose=True, device='cuda')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model has 3360601 parameters\n",
            "\n",
            "Epoch   1 | avg loss   0.5252 | train acc 72.99 | val acc 75.69\n",
            "Epoch   2 | avg loss   0.3528 | train acc 84.65 | val acc 79.47\n",
            "Epoch   3 | avg loss   0.2874 | train acc 88.04 | val acc 78.44\n",
            "Epoch   4 | avg loss   0.2574 | train acc 89.47 | val acc 80.85\n",
            "Epoch   5 | avg loss   0.2360 | train acc 90.55 | val acc 78.78\n",
            "Epoch   6 | avg loss   0.2147 | train acc 91.46 | val acc 82.00\n",
            "Epoch   7 | avg loss   0.1972 | train acc 92.25 | val acc 81.31\n",
            "Epoch   8 | avg loss   0.1806 | train acc 93.05 | val acc 80.62\n",
            "Epoch   9 | avg loss   0.1705 | train acc 93.39 | val acc 82.00\n",
            "Epoch  10 | avg loss   0.1605 | train acc 93.75 | val acc 81.77\n",
            "Stopping training because loss is NaN\n",
            "Final avg loss   0.1605 | final train acc 93.75 | best val acc 82.00 | train time 94 secs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5QENq5IsJCG"
      },
      "source": [
        "With a right implementation, you should be able to get $>$ 80% accuracy easily.\n",
        "\n",
        "So what are some conclusions?\n",
        "- CNN works well, but it doesn't seem to offer a dramatic improvement over CBOW. Both saturate around 80 accuracy.\n",
        "- CNN was easier to train because we used known hyperparameter values and also a more robust optimization method (Adam + weight decay)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_A6eIdBxuQGo"
      },
      "source": [
        "## Recurrent neural network (RNN) encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wb9XF_tUzcj3"
      },
      "source": [
        "An RNN should always be thought of as a mapping $\\textbf{RNN}_\\theta: (h_{t-1}, x_t) \\mapsto h_t$, which takes **previous hidden state** and **current input state** to compute a new hidden state (initial hidden state is set to zero, unless you want it to condition on some information). Thus we can think of applying an RNN to a sequence of word embeddings $x_1 \\ldots x_T$ as inducing a sequence of hidden states $h_1 \\ldots h_T$ by running it left-to-right. Note that it's getting its previous output as input, hence the name \"recurrent\". In particular, the last hidden state $h_T$ can be viewed as a very deep feedforward network in which each \"layer\"'s parameters are shared (i.e., RNN weights). An RNN is a natural way to handle variable-length input.\n",
        "\n",
        "One successful application of RNN is inducing *context-sensitive* word embeddings by running it left-to-right and right-to-left. Specifically, we compute\n",
        "$$\\begin{align*}\n",
        "\\textbf{RNN}_\\theta(x_1 \\ldots x_T) &= (h_1 \\ldots h_T) \\\\\n",
        "\\textbf{RNN}_\\phi(x_T \\ldots x_1) &= (h'_1 \\ldots h'_T)\n",
        "\\end{align*}$$\n",
        "And use\n",
        "$$\n",
        "z_t = \\mathrm{Concat}(h_t, h'_{T-t+1})\n",
        "$$\n",
        "as an embedding of the $t$-th token. Crucially, it's a function of all of its left tokens *and* all of its right tokens. We will use [LSTM](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html) as our choice of RNN (bidirectional LSTM, or **BiLSTM**), and compute these context-sensitive word embeddings then average them to obtain our final sentence embedding. Because of the recurrent nature of RNNs, we need optimization schemes like [packing variable-length sequences](https://stackoverflow.com/questions/51030782/why-do-we-pack-the-sequences-in-pytorch).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0V88SOw_8m47",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb96c4bf-1fbe-4c3a-a58d-954d3ac41047"
      },
      "source": [
        "dim_word = 4\n",
        "A = nn.Embedding(5, dim_word, padding_idx=0)\n",
        "x = A(torch.LongTensor([[1, 2, 3, 4, 0, 0, 0],\n",
        "                        [3, 3, 2, 2, 2, 1, 1]]))  # (batch_size, max_length, dim_word)\n",
        "lengths = torch.LongTensor([4, 7])\n",
        "\n",
        "dim_lstm = 20\n",
        "num_layers = 1\n",
        "bilstm = nn.LSTM(dim_word, dim_lstm, num_layers, bidirectional=True)  # By setting bidirectional, we're learning 2 LSTMs (forward and backward).\n",
        "\n",
        "# Pack padded sequences for compututational efficiency for RNNs.\n",
        "packed = nn.utils.rnn.pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n",
        "\n",
        "# Run BiLSTM.\n",
        "hiddens, (final_h, final_c) = bilstm(packed)\n",
        "\n",
        "# Undo packing\n",
        "hiddens, lengths = nn.utils.rnn.pad_packed_sequence(hiddens, batch_first=True)  # (batch_size, max_length, 2*dim_lstm)\n",
        "\n",
        "# Left/right context-sensitive token representations\n",
        "print('hiddens', hiddens.size())  # (batch_size, max_length, num_directions * dim_lstm)\n",
        "\n",
        "# Final hidden/cell states of LSTM\n",
        "print('final_h', final_h.size())  # (num_layers * num_directions, batch_size, dim_lstm)\n",
        "print('final_c', final_c.size())  # (num_layers * num_directions, batch_size, dim_lstm)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hiddens torch.Size([2, 7, 40])\n",
            "final_h torch.Size([2, 2, 20])\n",
            "final_c torch.Size([2, 2, 20])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXpwoAu1YTGa"
      },
      "source": [
        "class BiLSTMEncoder(nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size, dim_word=300, dim_lstm=300, num_layers=1, drop=0.5):\n",
        "    super().__init__()\n",
        "    self.dim = 2 * dim_lstm # This is the final dimension of a sentence embedding.\n",
        "    self.wemb = nn.Embedding(vocab_size, dim_word, padding_idx=0)  # Assumes padding token has index 0\n",
        "\n",
        "    self.bilstm = nn.LSTM(dim_word, dim_lstm, num_layers, bidirectional=True)\n",
        "    self.drop = nn.Dropout(drop)\n",
        "\n",
        "  def forward(self, sents, lengths):\n",
        "    # Embedding the words\n",
        "    x = self.wemb(sents)  # (batch_size, max_length, dim_word)\n",
        "\n",
        "    # Packing the word embeddings\n",
        "    packed = nn.utils.rnn.pack_padded_sequence(x, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
        "\n",
        "    # Running the BiLSTM\n",
        "    hiddens, (final_h, final_c) = self.bilstm(packed)\n",
        "\n",
        "    # Unpacking hidden states\n",
        "    hiddens, output_lengths = nn.utils.rnn.pad_packed_sequence(hiddens, batch_first=True)\n",
        "\n",
        "    sums = torch.sum(hiddens, dim=1)  # Sum over the time dimension\n",
        "    avgs = sums / output_lengths.view(-1, 1).float().to(sums.device)  # Averaging over non-padding elements\n",
        "\n",
        "    # Applying dropout\n",
        "    embs = self.drop(avgs)  # (batch_size, self.dim)\n",
        "\n",
        "    return embs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5BVjo7DDWGK"
      },
      "source": [
        "Let's train it without more tuning, using a similar configuration that we used for CNN."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJm5632-4bQD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9841c2c4-638a-4a53-e27b-26a33d691cd0"
      },
      "source": [
        "if True:  # Set True to run. We will use this model later for error analysis.\n",
        "  set_seed(42)\n",
        "  model_bilstm = BinaryClassifier(BiLSTMEncoder(vocab_size))\n",
        "  print('Model has {} parameters\\n'.format(count_params(model_bilstm)))\n",
        "  _ = train(model_bilstm, DataLoader(dataset_train, batch_size=64, shuffle=True),\n",
        "            torch.optim.Adam(model_bilstm.parameters(), lr=0.001, weight_decay=0.0003), clip=1., num_epochs=30, verbose=True, device='cuda', select_model=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model has 4445401 parameters\n",
            "\n",
            "Epoch   1 | avg loss   0.5087 | train acc 74.25 | val acc 76.95\n",
            "Epoch   2 | avg loss   0.3536 | train acc 84.33 | val acc 78.90\n",
            "Epoch   3 | avg loss   0.2885 | train acc 88.03 | val acc 82.22\n",
            "Epoch   4 | avg loss   0.2597 | train acc 89.47 | val acc 81.54\n",
            "Epoch   5 | avg loss   0.2425 | train acc 90.25 | val acc 82.34\n",
            "Epoch   6 | avg loss   0.2273 | train acc 91.07 | val acc 83.49\n",
            "Epoch   7 | avg loss   0.2176 | train acc 91.43 | val acc 83.26\n",
            "Epoch   8 | avg loss   0.2082 | train acc 91.83 | val acc 82.45\n",
            "Epoch   9 | avg loss   0.2015 | train acc 92.19 | val acc 79.82\n",
            "Stopping training because loss is NaN\n",
            "Final avg loss   0.2015 | final train acc 92.19 | best val acc 83.49 | train time 135 secs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmK6yZsqDf-v"
      },
      "source": [
        "Again, with a right implementation, you should be able to get $>$ 80% accuracy easily. It's clear that the BiLSTM encoder learns effective representations.\n",
        "\n",
        "- **Pros**: Natural sequence handling. Function of the order of the entire sequence.\n",
        "- **Cons**: Must compute previous states before computing next states. The recurrent nature makes it impossible to parallelize RNNs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCw8Wdj6UjEn"
      },
      "source": [
        "# Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vek8qX448Ajn"
      },
      "source": [
        "TODO: Fill in the table below with the *best* validation accuracy you could get for each type of encoder. Also record other associated quantities like number of parameters and final training loss.\n",
        "\n",
        "| Encoder   | Num Parameters  |   Final Avg Loss | Final Train Acc | Val Acc  |\n",
        "| :---:     | :---:           |:---:             | :---:           | :---:    |\n",
        "| CBOW      | 644225           |  0.2540 | 90.00 | 80.62          |          \n",
        "| CNN       | 3360601           |  0.1605  | 93.75| 82.00 |\n",
        "| BiLSTM    | 4445401         | 0.2015         |    92.19       | 83.49  |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqPvDZqnUkvw"
      },
      "source": [
        "# Error Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTIiYfym7nA5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7398100-f672-4b0d-a94e-411d7ac89986"
      },
      "source": [
        "def analyze(model):\n",
        "  wrongs = []\n",
        "  rights = []\n",
        "  model.eval()  # This turns off the training mode.\n",
        "  sent_ind = 0\n",
        "  with torch.no_grad():  # This deactivates autodiff for improved efficiency.\n",
        "    for batch in dataloader_val:\n",
        "      sents, labels, lengths = batch\n",
        "      sents = sents.to(model.score.weight.device)  # Send data to same device that model is on.\n",
        "      labels = labels.to(model.score.weight.device)\n",
        "      lengths = lengths.to(model.score.weight.device)\n",
        "      logits, _ = model(sents, lengths, labels)\n",
        "      probs = torch.sigmoid(logits)\n",
        "      preds = torch.where(logits > 0., 1, 0)  # 1 if p(1|x) > 0.5, 0 else\n",
        "      corrects = (preds == labels)\n",
        "      for i in range(corrects.size(0)):\n",
        "        if corrects[i] == 0:\n",
        "          wrongs.append((sent_ind, probs[i].item(), labels[i].item()))\n",
        "        else:\n",
        "          rights.append((sent_ind, probs[i].item(), labels[i].item()))\n",
        "        sent_ind += 1\n",
        "  return wrongs, rights\n",
        "\n",
        "# We'll assume model_bilstm saved from the RNN section.\n",
        "wrongs, rights = analyze(model_bilstm)\n",
        "assert len(wrongs) + len(rights) == len(val_sents)\n",
        "set_seed(42)\n",
        "samples_right = random.sample(rights, 10)\n",
        "samples_wrong = random.sample(wrongs, 10)\n",
        "print('Model accuracy: {:2.2f}'.format(get_acc_val(model_bilstm)))\n",
        "\n",
        "print('\\nGot these right')\n",
        "for sent_ind, prob, label in samples_right:\n",
        "  print('{} (model positive prob {:5.5f}): {}'.format(label2str[label], prob, ' '.join([vocab[word_index] for word_index in val_sents[sent_ind]])))\n",
        "\n",
        "print('\\nGot these wrong')\n",
        "for sent_ind, prob, label in samples_wrong:\n",
        "  print('{} (model positive prob {:5.5f}): {}'.format(label2str[label], prob, ' '.join([vocab[word_index] for word_index in val_sents[sent_ind]])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model accuracy: 83.49\n",
            "\n",
            "Got these right\n",
            "NEGATIVE (model positive prob 0.38760): at once half-baked and <unk> .\n",
            "NEGATIVE (model positive prob 0.39050): sacrifices the value of its <unk> of <unk> <unk> with its <unk> <unk> .\n",
            "POSITIVE (model positive prob 0.99801): it 's an offbeat treat that pokes fun at the <unk> exercise while also <unk> its significance for those who take part .\n",
            "NEGATIVE (model positive prob 0.01052): no way i can believe this load of junk .\n",
            "POSITIVE (model positive prob 0.92785): a <unk> constructed , highly <unk> film , and an audacious return to form that can comfortably sit among <unk> godard 's finest work .\n",
            "NEGATIVE (model positive prob 0.05721): basically a static series of <unk> ( and <unk> ) <unk> between the stars .\n",
            "POSITIVE (model positive prob 0.88973): if you <unk> on david mamet 's mind tricks ... rent this movie and enjoy !\n",
            "POSITIVE (model positive prob 0.90567): <unk> ... <unk> a lot of energy into his nicely nuanced narrative and <unk> himself with a cast of quirky -- but not <unk> -- street characters .\n",
            "NEGATIVE (model positive prob 0.00200): the movie , directed by mick jackson , leaves no cliche unturned , from the predictable plot to the characters straight out of central casting .\n",
            "NEGATIVE (model positive prob 0.41034): <unk> <unk> every <unk> trick to give us the <unk> .\n",
            "\n",
            "Got these wrong\n",
            "POSITIVE (model positive prob 0.13983): turns potentially forgettable formula into something strangely diverting .\n",
            "POSITIVE (model positive prob 0.04444): the jabs it employs are short , carefully <unk> and <unk> .\n",
            "NEGATIVE (model positive prob 0.71340): it 's one <unk> world when even <unk> <unk> around group therapy sessions .\n",
            "POSITIVE (model positive prob 0.11884): one of creepiest , scariest movies to come along in a long , long time , easily <unk> blair witch or the others .\n",
            "POSITIVE (model positive prob 0.36256): ( lawrence <unk> ) all over the stage , dancing , running , <unk> , <unk> his face and generally <unk> the wacky talent that brought him fame in the first place .\n",
            "POSITIVE (model positive prob 0.40848): the story and structure are <unk> .\n",
            "POSITIVE (model positive prob 0.48334): a painfully funny ode to bad behavior .\n",
            "NEGATIVE (model positive prob 0.86111): it 's somewhat clumsy and too <unk> paced -- but its story about a mysterious creature with <unk> <unk> offers a solid <unk> , a terrific climax , and some nice chills along the way .\n",
            "NEGATIVE (model positive prob 0.61531): this one is definitely one to skip , even for horror movie fanatics .\n",
            "NEGATIVE (model positive prob 0.95656): a subject like this should inspire reaction in its audience ; the pianist does not .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enmdE0VebESO"
      },
      "source": [
        "It's a little difficult to tell with certainty, but generally the examples that the model gets correct seem \"easier\". They often have key phrases like *miserable*, *plot holes*, *formulaic* for negative and *delivers*, *thrilling* for positive, and there's no sentiment flipping. Indeed the model's prediction probabilities are very skewed (either close to 0 or 1) in these cases. In contrast, the examples that the model gets incorrect seem a bit more subtle (e.g., *a better title , for all concerned , might be swept under the rug .* as negative), and the model is less certain (probabilities more often around 0.5)."
      ]
    }
  ]
}